{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XRxHiKdGHiT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coursework 2: Image segmentation\n",
    "\n",
    "In this coursework you will develop and train a convolutional neural network for brain tumour image segmentation. Please read both the text and the code in this notebook to get an idea what you are expected to implement. Pay attention to the missing code blocks that look like this:\n",
    "\n",
    "```\n",
    "### Insert your code ###\n",
    "...\n",
    "### End of your code ###\n",
    "```\n",
    "\n",
    "## What to do?\n",
    "\n",
    "* Complete and run the code using `jupyter-lab` or `jupyter-notebook` to get the results.\n",
    "\n",
    "* Export (File | Save and Export Notebook As...) the notebook as a PDF file, which contains your code, results and answers, and upload the PDF file onto [Scientia](https://scientia.doc.ic.ac.uk).\n",
    "\n",
    "* Instead of clicking the Export button, you can also run the following command instead: `jupyter nbconvert coursework.ipynb --to pdf`\n",
    "\n",
    "* If Jupyter complains about some problems in exporting, it is likely that pandoc (https://pandoc.org/installing.html) or latex is not installed, or their paths have not been included. You can install the relevant libraries and retry.\n",
    "\n",
    "* If Jupyter-lab does not work for you at the end, you can use Google Colab to write the code and export the PDF file.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "You need to install Jupyter-Lab (https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) and other libraries used in this coursework, such as by running the command:\n",
    "`pip3 install [package_name]`\n",
    "\n",
    "## GPU resource\n",
    "\n",
    "The coursework is developed to be able to run on CPU, as all images have been pre-processed to be 2D and of a smaller size, compared to original 3D volumes.\n",
    "\n",
    "However, to save training time, you may want to use GPU. In that case, you can run this notebook on Google Colab. On Google Colab, go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor. At the end, please still export everything and submit as a PDF file on Scientia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# These libraries should be sufficient for this tutorial.\n",
    "# However, if any other library is needed, please install by yourself.\n",
    "\n",
    "import tarfile\n",
    "import imageio as imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Download and visualise the imaging dataset.\n",
    "\n",
    "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
    "\n",
    "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
    "\n",
    "- 0: background\n",
    "- 1: edema\n",
    "- 2: non-enhancing tumour\n",
    "- 3: enhancing tumour"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!pwd\n",
    "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
    "\n",
    "# Unzip the '.tar.gz' file to the current directory\n",
    "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
    "datafile.extractall()\n",
    "datafile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise a random set of 4 training images along with their label maps.\n",
    "\n",
    "Suggested colour map for brain MR image:\n",
    "```\n",
    "cmap = 'gray'\n",
    "```\n",
    "\n",
    "Suggested colour map for segmentation map:\n",
    "```\n",
    "cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Insert your code ###\n",
    "import glob\n",
    "\n",
    "imgs = glob.glob(\"Task01_BrainTumour_2D/training_images/*\")\n",
    "short = random.sample(imgs, 4)\n",
    "for i, img in enumerate(short):\n",
    "    image = imageio.imread(img)\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "### End of your code ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implement a dataset class.\n",
    "\n",
    "It can read the imaging dataset and get items, pairs of images and label maps, as training batches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalise_intensity(image, thres_roi=1.0):\n",
    "    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n",
    "    # ROI defines the image foreground\n",
    "    val_l = np.percentile(image, thres_roi)\n",
    "    roi = (image >= val_l)\n",
    "    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n",
    "    eps = 1e-6\n",
    "    image2 = (image - mu) / (sigma + eps)\n",
    "    return image2\n",
    "\n",
    "\n",
    "class BrainImageSet(Dataset):\n",
    "    \"\"\" Brain image set \"\"\"\n",
    "    def __init__(self, image_path, label_path='', deploy=False):\n",
    "        self.image_path = image_path\n",
    "        self.deploy = deploy\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        image_names = sorted(os.listdir(image_path))\n",
    "        for image_name in image_names:\n",
    "            # Read the image\n",
    "            image: np.ndarray = imageio.imread(os.path.join(image_path, image_name))\n",
    "            # print(image.shape)\n",
    "            self.images += [image]\n",
    "\n",
    "            # Read the label map\n",
    "            if not self.deploy:\n",
    "                label_name = os.path.join(label_path, image_name)\n",
    "                label = imageio.imread(label_name)\n",
    "                self.labels += [label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get an image and perform intensity normalisation\n",
    "        # Dimension: XY\n",
    "        image = normalise_intensity(self.images[idx])\n",
    "\n",
    "        # Get its label map\n",
    "        # Dimension: XY\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        # Get a batch of paired images and label maps\n",
    "        # Dimension of images: NCXY\n",
    "        # Dimension of labels: NXY\n",
    "        images, labels = [], []\n",
    "\n",
    "        ### Insert your code ###\n",
    "        n = self.__len__()\n",
    "        ixs = np.arange(0, n, 1)\n",
    "        rng = np.random.default_rng()\n",
    "        selected = rng.choice(a=ixs, size=batch_size, replace=False)\n",
    "        images, labels = np.array(self.images), np.array(self.labels)\n",
    "        images, labels = images[selected], labels[selected]\n",
    "        for i in range(len(images)):\n",
    "            images[i] = normalise_intensity(images[i])\n",
    "        images = images[:, np.newaxis, :, :]\n",
    "        ### End of your code ###\n",
    "        return images, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Build a U-net architecture.\n",
    "\n",
    "You will implement a U-net architecture. If you are not familiar with U-net, please read this paper:\n",
    "\n",
    "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n",
    "\n",
    "For the first convolutional layer, you can start with 16 filters. We have implemented the encoder path. Please complete the decoder path."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" U-net \"\"\"\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # BatchNorm: by default during training this layer keeps running estimates\n",
    "        # of its computed mean and variance, which are then used for normalization\n",
    "        # during evaluation.\n",
    "\n",
    "        # Encoder path\n",
    "        n = num_filter  # 16\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 32\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 64\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 128\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder path\n",
    "        ### Insert your code ###\n",
    "        n *= 2 # 256\n",
    "        self.bot = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n, int(n / 2), kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(int(n / 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n //= 2  # 128\n",
    "        self.deco1 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n, int(n / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(int(n / 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n //= 2  # 64\n",
    "        self.deco2 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n, int(n / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(int(n / 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n //= 2  # 32\n",
    "        self.deco3 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n, int(n / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(int(n / 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n //= 2  # 16\n",
    "        self.deco4 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, output_channel, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ### End of your code ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional operators defined above to build the U-net\n",
    "        # The encoder part is already done for you.\n",
    "        # You need to complete the decoder part.\n",
    "        # Encoder\n",
    "        x = self.conv1(x)\n",
    "        conv1_skip = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        conv2_skip = x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        conv3_skip = x\n",
    "\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        # Decoder\n",
    "        ### Insert your code ###\n",
    "        conv4_skip = x\n",
    "        x = self.bot(x)\n",
    "\n",
    "        x = torch.cat((x, conv4_skip), dim=1)\n",
    "        x = self.deco1(x)\n",
    "\n",
    "        x = torch.cat((x, conv3_skip), dim=1)\n",
    "        x = self.deco2(x)\n",
    "\n",
    "        x = torch.cat((x, conv2_skip), dim=1)\n",
    "        x = self.deco3(x)\n",
    "\n",
    "        x = torch.cat((x, conv1_skip), dim=1)\n",
    "        x = self.deco4(x)\n",
    "        ### End of your code ###\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Train the segmentation model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# CUDA device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "# Build the model\n",
    "num_class = 4\n",
    "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
    "model = model.to(device)\n",
    "params = list(model.parameters())\n",
    "\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(params, lr=1e-3)\n",
    "\n",
    "# Segmentation loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Datasets\n",
    "train_set = BrainImageSet('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
    "test_set = BrainImageSet('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
    "\n",
    "# Train the model\n",
    "# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\n",
    "num_iter = 15_000\n",
    "num_switch = 10_000\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "jitter = T.ColorJitter(brightness=.5, hue=.3, contrast=1, saturation=1)\n",
    "start = time.time()\n",
    "for it in tqdm(range(1, 1 + num_iter)):\n",
    "    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n",
    "    start_iter = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # Get a batch of images and labels\n",
    "    images, labels = train_set.get_random_batch(train_batch_size)\n",
    "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "    if it >= num_switch:\n",
    "      for i in range(len(images)):\n",
    "        images[i] = jitter(images[i])\n",
    "    logits = model(images)\n",
    "\n",
    "    # Perform optimisation and print out the training loss\n",
    "    ### Insert your code ###\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ### End of your code ###\n",
    "\n",
    "    # Evaluate\n",
    "    if it % (num_iter // 100) == 0:\n",
    "        model.eval()\n",
    "        # Disabling gradient calculation during reference to reduce memory consumption\n",
    "        with torch.no_grad():\n",
    "            # Evaluate on a batch of test images and print out the test loss\n",
    "            ### Insert your code ###\n",
    "            images, labels = test_set.get_random_batch(train_batch_size)\n",
    "            images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "            images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "            logits = model(images)\n",
    "            loss_test = criterion(logits, labels)\n",
    "            print(f'{loss.item():.3f}', f'{loss_test.item():.3f}')\n",
    "        \n",
    "    if it == num_iter:\n",
    "      model.eval()\n",
    "      # Disabling gradient calculation during reference to reduce memory consumption\n",
    "      with torch.no_grad():\n",
    "          # Evaluate on a batch of test images and print out the test loss\n",
    "          ### Insert your code ###\n",
    "          images, labels = test_set.get_random_batch(len(test_set))\n",
    "          images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "          images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "          logits = model(images)\n",
    "          loss_test = criterion(logits, labels)\n",
    "          print('Loss on whole test set:', f'{loss_test.item():.3f}')\n",
    "          ### End of your code ###\n",
    "\n",
    "    # Save the model\n",
    "    if it % 5000 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\n",
    "print('Training took {:.3f}s in total.'.format(time.time() - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Deploy the trained model to a random set of 4 test images and visualise the automated segmentation.\n",
    "\n",
    "You can show the images as a 4 x 3 panel. Each row shows one example, with the 3 columns being the test image, automated segmentation and ground truth segmentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Insert your code ###\n",
    "images, labels = test_set.get_random_batch(4)\n",
    "images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "model.train(False)\n",
    "logits = model(images)\n",
    "results = torch.argmax(logits, 1)\n",
    "foo = zip(images, results, labels)\n",
    "\n",
    "cmap = colors.ListedColormap(['white', 'green', 'yellow', 'red'])\n",
    "\n",
    "for i, (test, auto, ground) in enumerate(foo):\n",
    "    test = torch.squeeze(test)\n",
    "    plt.subplot(4, 3, 3 * i + 1)\n",
    "    plt.imshow(test.cpu(), cmap='gray')\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "\n",
    "    auto = torch.squeeze(auto)\n",
    "    plt.subplot(4, 3, 3 * i + 2)\n",
    "    plt.imshow(auto.cpu(), cmap=cmap)\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "\n",
    "    ground = torch.squeeze(ground)\n",
    "    plt.subplot(4, 3, 3 * i + 3)\n",
    "    plt.imshow(ground.cpu(), cmap=cmap)\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "### End of your code ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Discussion. Does your trained model work well? How would you improve this model so it can be deployed to the real clinic?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluation**\n",
    "\n",
    "The model works reasonably well. The mean cross-entropy loss evaluated on the whole test set (code cell below) is 0.061. In the majority of cases it places the non-background classes (edema and tumours) in the right place. However, the shape is usually not the same as the ground truth and the non-background classes are a bit shifted. The model often confuses tumours and edema. Sometimes the model doesn't detect a tumour where there actually is one and vice versa.\n",
    "\n",
    "**Future improvements**\n",
    "\n",
    "In the future the following improvements could be made to increase the accuracy:\n",
    "* Data augmentation + more iterations (grid distortion, and elastic transform). The current model uses augmentation in the form of color tranformation (ColorJitter) but other types could be added.\n",
    "* More training data\n",
    "* Use UNet++ architecture (insert convolution layers between skip connections)\n",
    "* Add instance segmentation to differentiate between individual tumour clusters\n",
    "* Add some form of early stopping, L2 regularisation or weight pruning to prevent overfitting\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate accuracy on whole test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "num_class = 4\n",
    "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
    "model.load_state_dict(torch.load('saved_models/model_15000.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Disabling gradient calculation during reference to reduce memory consumption\n",
    "with torch.no_grad():\n",
    "    # Evaluate on a batch of test images and print out the test loss\n",
    "    ### Insert your code ###\n",
    "    print(len(test_set))\n",
    "    images, labels = test_set.get_random_batch(len(test_set))\n",
    "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "    logits = model(images)\n",
    "    loss_test = criterion(logits, labels)\n",
    "    print('Loss on whole test set:', f'{loss_test.item():.3f}')\n",
    "    ### End of your code ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "num_class = 4\n",
    "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
    "model.load_state_dict(torch.load('saved_models/model_15000.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Disabling gradient calculation during reference to reduce memory consumption\n",
    "with torch.no_grad():\n",
    "    # Evaluate on a batch of test images and print out the test loss\n",
    "    ### Insert your code ###\n",
    "    print(len(test_set))\n",
    "    images, labels = test_set.get_random_batch(len(test_set))\n",
    "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "    logits = model(images)\n",
    "    loss_test = criterion(logits, labels)\n",
    "    print('Loss on whole test set:', f'{loss_test.item():.3f}')\n",
    "    ### End of your code ###"
   ],
   "metadata": {
    "id": "HKRrkP2TYtQb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "0cf4a5d9-7606-4e2c-b6e6-02063805147f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "730\n",
      "Loss on whole test set: 0.065\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "60006",
   "language": "python",
   "display_name": "60006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}